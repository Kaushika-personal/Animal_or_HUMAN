{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d06c77e4d479422187ea64456ff48f19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da5ee74eecc44f3a87ce70c93d18dfa1",
              "IPY_MODEL_67a526f8b833440cb0d30c1c22971b4a",
              "IPY_MODEL_0160bc1c7b5b4e3a8cf0544e8082e7ce"
            ],
            "layout": "IPY_MODEL_2088bb46029742e8922f1e9462de823a"
          }
        },
        "da5ee74eecc44f3a87ce70c93d18dfa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fed6b01294b4976b80d77fddb10798b",
            "placeholder": "​",
            "style": "IPY_MODEL_758f29690b604857b0317504a704ee91",
            "value": "model-00002-of-00003.safetensors: 100%"
          }
        },
        "67a526f8b833440cb0d30c1c22971b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21534abfb5154cbd96fa65e86131bc24",
            "max": 1993062424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb056b8ac6dd4a7b98813962ef949417",
            "value": 1993062424
          }
        },
        "0160bc1c7b5b4e3a8cf0544e8082e7ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dd40a498f7e4582bf150bac8d6018bc",
            "placeholder": "​",
            "style": "IPY_MODEL_3025a95ef70b4417a7b46e725aa0f424",
            "value": " 1.99G/1.99G [01:08&lt;00:00, 29.4MB/s]"
          }
        },
        "2088bb46029742e8922f1e9462de823a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fed6b01294b4976b80d77fddb10798b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "758f29690b604857b0317504a704ee91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21534abfb5154cbd96fa65e86131bc24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb056b8ac6dd4a7b98813962ef949417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1dd40a498f7e4582bf150bac8d6018bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3025a95ef70b4417a7b46e725aa0f424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67bd2c3658564480b0be83099abce50c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f86c579ef2204b70b497e73de7d55aea",
              "IPY_MODEL_f32a0004ca5c4a06b62bd953343f4709",
              "IPY_MODEL_e3bd08890d0d4ec1b3792ff4ceb0952a"
            ],
            "layout": "IPY_MODEL_89281cb9a8134ba8bccd4b30d986c43c"
          }
        },
        "f86c579ef2204b70b497e73de7d55aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b504eff049974838a7ea541059a9481a",
            "placeholder": "​",
            "style": "IPY_MODEL_e256ac32142e4528a966636f43ae7df3",
            "value": "Upload 4 LFS files: 100%"
          }
        },
        "f32a0004ca5c4a06b62bd953343f4709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e59337ce792e48ab9a72b66038c1dba6",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d9eee60cf324eaa88e5137aa179a518",
            "value": 4
          }
        },
        "e3bd08890d0d4ec1b3792ff4ceb0952a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6946380fe163449c99f1bb09e5713cf3",
            "placeholder": "​",
            "style": "IPY_MODEL_7896d3016a834a6dbde1c78d2fafd087",
            "value": " 4/4 [01:08&lt;00:00, 29.30s/it]"
          }
        },
        "89281cb9a8134ba8bccd4b30d986c43c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b504eff049974838a7ea541059a9481a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e256ac32142e4528a966636f43ae7df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e59337ce792e48ab9a72b66038c1dba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d9eee60cf324eaa88e5137aa179a518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6946380fe163449c99f1bb09e5713cf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7896d3016a834a6dbde1c78d2fafd087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3b9cc0286e04033914b8a005e2eab94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fb7f82f7e084b9d8fb1fae0af81e70b",
              "IPY_MODEL_552bb27b28ae44209b187223b2262409",
              "IPY_MODEL_e7478cd26fcc43a7b1d20a6d14e27f5b"
            ],
            "layout": "IPY_MODEL_7038fb211bcb43e38a5cd457856aee58"
          }
        },
        "6fb7f82f7e084b9d8fb1fae0af81e70b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44a3e78a2b5247eda71b16c9521f7141",
            "placeholder": "​",
            "style": "IPY_MODEL_9e2d0c6f64954a2c86e22632a4ae0d3f",
            "value": "model-00001-of-00003.safetensors: 100%"
          }
        },
        "552bb27b28ae44209b187223b2262409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_982abbe7cce749ad9d3cb733517bf75e",
            "max": 1995539928,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1fcaf8888854c269ed1406c6130ee83",
            "value": 1995539928
          }
        },
        "e7478cd26fcc43a7b1d20a6d14e27f5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d15ecd97393b4feaa2bf4776e7ddd978",
            "placeholder": "​",
            "style": "IPY_MODEL_0375cd40ba2b4e20b438bb84edac8d36",
            "value": " 2.00G/2.00G [01:01&lt;00:00, 21.2MB/s]"
          }
        },
        "7038fb211bcb43e38a5cd457856aee58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44a3e78a2b5247eda71b16c9521f7141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e2d0c6f64954a2c86e22632a4ae0d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "982abbe7cce749ad9d3cb733517bf75e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1fcaf8888854c269ed1406c6130ee83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d15ecd97393b4feaa2bf4776e7ddd978": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0375cd40ba2b4e20b438bb84edac8d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08af4eff77c6477d958f46f9e1317367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44101207a6d54004995e1be9b51f27cd",
              "IPY_MODEL_64dad4b97a8a45689d9c6582994e822f",
              "IPY_MODEL_f64697690e954d0c8c6ca74345ad5199"
            ],
            "layout": "IPY_MODEL_3e6652e91969458bb31db2c26875349f"
          }
        },
        "44101207a6d54004995e1be9b51f27cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5e6802660a04195974840b376ee3759",
            "placeholder": "​",
            "style": "IPY_MODEL_f5c09a4186324911ab5bf3b76c5855fa",
            "value": "model-00003-of-00003.safetensors: 100%"
          }
        },
        "64dad4b97a8a45689d9c6582994e822f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f96c235e1ae84048b12c1a26d0be0b13",
            "max": 429448016,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74e4c67f72964824b83d9512499f5c35",
            "value": 429448016
          }
        },
        "f64697690e954d0c8c6ca74345ad5199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_400a2d4ed1a4404989894296d25e0ddb",
            "placeholder": "​",
            "style": "IPY_MODEL_c759faeeeb6848d3a875df9421194b76",
            "value": " 429M/429M [00:12&lt;00:00, 42.6MB/s]"
          }
        },
        "3e6652e91969458bb31db2c26875349f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5e6802660a04195974840b376ee3759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5c09a4186324911ab5bf3b76c5855fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f96c235e1ae84048b12c1a26d0be0b13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74e4c67f72964824b83d9512499f5c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "400a2d4ed1a4404989894296d25e0ddb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c759faeeeb6848d3a875df9421194b76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b040f4696764352b49ed8f2cdfd90af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7abe6e4d8f274d26947603e494e47751",
              "IPY_MODEL_f8b75ed557af4c99a719f891bd8fe5c1",
              "IPY_MODEL_7edcf4d8308b4954af46a11bcd1e28cb"
            ],
            "layout": "IPY_MODEL_c197e4d23a664057ae40e815ff525fef"
          }
        },
        "7abe6e4d8f274d26947603e494e47751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_754b55f1916f42ce9c25657c6ac4ba1b",
            "placeholder": "​",
            "style": "IPY_MODEL_a41d129f5f51412ab90fd96a23c5abca",
            "value": "tokenizer.json: 100%"
          }
        },
        "f8b75ed557af4c99a719f891bd8fe5c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6e857ed22a345b8b0361a31f0d89ff8",
            "max": 11420371,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78d065e7b1814c28bae7e89799ae55bf",
            "value": 11420371
          }
        },
        "7edcf4d8308b4954af46a11bcd1e28cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ec01fc297ab4a5eacd898a3c0fa1ab7",
            "placeholder": "​",
            "style": "IPY_MODEL_665d56b3329f40c084e60cc6fa56f574",
            "value": " 11.4M/11.4M [00:00&lt;00:00, 29.2MB/s]"
          }
        },
        "c197e4d23a664057ae40e815ff525fef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "754b55f1916f42ce9c25657c6ac4ba1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a41d129f5f51412ab90fd96a23c5abca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6e857ed22a345b8b0361a31f0d89ff8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78d065e7b1814c28bae7e89799ae55bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ec01fc297ab4a5eacd898a3c0fa1ab7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "665d56b3329f40c084e60cc6fa56f574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63863af80d2b45f08015c93c6f993d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96c6d00e1db74f00a371b286365e2a16",
              "IPY_MODEL_a84ed58830ad4d84a977c391bfaa2096",
              "IPY_MODEL_cff29d93d1d2497da7ad665941617c44"
            ],
            "layout": "IPY_MODEL_a956922642fa4df2814658006b23f375"
          }
        },
        "96c6d00e1db74f00a371b286365e2a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9ac6b1e2d4f4fd0a96d27aa9905a35c",
            "placeholder": "​",
            "style": "IPY_MODEL_a0ec4a6181e14bf290580ded76f02bc5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a84ed58830ad4d84a977c391bfaa2096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f932d94fca0d4fa19972f5bb56a9edce",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a545bc30d2ea44d48a7b4d4e7d09a768",
            "value": 3
          }
        },
        "cff29d93d1d2497da7ad665941617c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b9a232982bd4595bd66de0a153bfd79",
            "placeholder": "​",
            "style": "IPY_MODEL_d9d3073d743a4c0e9400b56fac43ca01",
            "value": " 3/3 [00:00&lt;00:00,  6.42it/s]"
          }
        },
        "a956922642fa4df2814658006b23f375": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9ac6b1e2d4f4fd0a96d27aa9905a35c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0ec4a6181e14bf290580ded76f02bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f932d94fca0d4fa19972f5bb56a9edce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a545bc30d2ea44d48a7b4d4e7d09a768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b9a232982bd4595bd66de0a153bfd79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9d3073d743a4c0e9400b56fac43ca01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d41016bf4024e3baca2ffab55c0abd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_089eff09c5574737a198129f7b3f5d64",
              "IPY_MODEL_1f87ba8b996d49858db7413da2b69bde",
              "IPY_MODEL_e7d828c9ceb94fa3a624f7d8086ae6ae"
            ],
            "layout": "IPY_MODEL_a5ae59e3e04b431a961a3d2825b47dce"
          }
        },
        "089eff09c5574737a198129f7b3f5d64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_265c9de21cdf46d6a9cc50abeeeda9c1",
            "placeholder": "​",
            "style": "IPY_MODEL_63cec27bd6d547a4b308449600822ad7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1f87ba8b996d49858db7413da2b69bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dc87543e69d458a9e4e2cf4e92fb9fd",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_653a6f1f931846d29a2c84314d767d5d",
            "value": 3
          }
        },
        "e7d828c9ceb94fa3a624f7d8086ae6ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7f5343ef53545ac925fc06deb294186",
            "placeholder": "​",
            "style": "IPY_MODEL_825cba81c72044b49fb26db934f7d853",
            "value": " 3/3 [00:01&lt;00:00,  1.80it/s]"
          }
        },
        "a5ae59e3e04b431a961a3d2825b47dce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "265c9de21cdf46d6a9cc50abeeeda9c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63cec27bd6d547a4b308449600822ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2dc87543e69d458a9e4e2cf4e92fb9fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "653a6f1f931846d29a2c84314d767d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7f5343ef53545ac925fc06deb294186": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "825cba81c72044b49fb26db934f7d853": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9dfcd0da60a461e8c834a7b2c07ced4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9dc08fc29e8d4967b6f5ceb4b9df4ccd",
              "IPY_MODEL_b51873f397984dedacce5aab1af426df",
              "IPY_MODEL_c3d37f84d3404236bc23e5373da3aac8"
            ],
            "layout": "IPY_MODEL_6376d4af70d94514b37186fbf828b841"
          }
        },
        "9dc08fc29e8d4967b6f5ceb4b9df4ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41786e22fa214a95b382191bdc0d389b",
            "placeholder": "​",
            "style": "IPY_MODEL_4f0b7a2b5b2f4949b5ec6a8938407d21",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b51873f397984dedacce5aab1af426df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2475359adf8742eda908218d8a433978",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d35bcded09a4ddb9b331eda6d075db7",
            "value": 3
          }
        },
        "c3d37f84d3404236bc23e5373da3aac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95726b06b8334b7ea156f743f7ca6f54",
            "placeholder": "​",
            "style": "IPY_MODEL_116643b7eb1b40b5af39fb30e842effa",
            "value": " 3/3 [00:00&lt;00:00,  3.82it/s]"
          }
        },
        "6376d4af70d94514b37186fbf828b841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41786e22fa214a95b382191bdc0d389b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f0b7a2b5b2f4949b5ec6a8938407d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2475359adf8742eda908218d8a433978": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d35bcded09a4ddb9b331eda6d075db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95726b06b8334b7ea156f743f7ca6f54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "116643b7eb1b40b5af39fb30e842effa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "h_R7MKckgOnQ",
        "outputId": "2634c8db-dc25-4ea1-bb53-d3895d0897ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-9m8n7l4r\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-9m8n7l4r\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit a42ba80fa520c784c8f11a973ca9034e5f859b79\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (2025.1.31)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.52.0.dev0-py3-none-any.whl size=11448197 sha256=4f369cd8c30f690c810d7072fdb1d1a6e98387c4bced9dd21cbb457e545fc4b3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sxmyjw0v/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llamafactory 0.9.3.dev0 requires transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0, but you have transformers 4.52.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed transformers-4.52.0.dev0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "047ea74be337408bb6f6442ed1121736"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSJTQmUMgcoo",
        "outputId": "1fd02d6c-0325-4a1c-8649-d0edfabc2f21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 23004, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 23004 (delta 28), reused 11 (delta 8), pack-reused 22949 (from 2)\u001b[K\n",
            "Receiving objects: 100% (23004/23004), 46.28 MiB | 29.33 MiB/s, done.\n",
            "Resolving deltas: 100% (16768/16768), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LLaMA-Factory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijY1lxlYtQXB",
        "outputId": "8411d81f-7b65-42a6-b89f-d1b180a11109"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsuI-Z0vtSd5",
        "outputId": "c42f043d-14dc-46f7-8542-1223c272a31d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (4.51.3)\n",
            "Collecting datasets<=3.5.0,>=2.16.0 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: accelerate<=1.6.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.5.2)\n",
            "Requirement already satisfied: peft<=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.14.0)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from -r requirements.txt (line 5))\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.21.1)\n",
            "Collecting gradio<=5.25.0,>=4.38.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-5.25.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.14.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (0.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (0.2.0)\n",
            "Collecting tiktoken (from -r requirements.txt (line 11))\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (5.29.4)\n",
            "Collecting uvicorn (from -r requirements.txt (line 13))\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting fastapi (from -r requirements.txt (line 14))\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting sse-starlette (from -r requirements.txt (line 15))\n",
            "  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (3.10.0)\n",
            "Collecting fire (from -r requirements.txt (line 17))\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (6.0.2)\n",
            "Collecting numpy<2.0.0 (from -r requirements.txt (line 20))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<=2.10.6 (from -r requirements.txt (line 21))\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (2.2.2)\n",
            "Collecting av (from -r requirements.txt (line 23))\n",
            "  Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (0.11.0)\n",
            "Collecting tyro<0.9.0 (from -r requirements.txt (line 25))\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (0.30.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2))\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (2.6.0+cu124)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7))\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (4.9.0)\n",
            "Collecting ffmpy (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7))\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7))\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (3.10.16)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (11.1.0)\n",
            "Collecting pydub (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7))\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7))\n",
            "  Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7))\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7))\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7))\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (4.13.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (15.0.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->-r requirements.txt (line 13)) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->-r requirements.txt (line 13)) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (2.8.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->-r requirements.txt (line 17)) (3.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->-r requirements.txt (line 21)) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->-r requirements.txt (line 21))\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 22)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 22)) (2025.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 24)) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 24)) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 24)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 24)) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 24)) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 24)) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 24)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 24)) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 24)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 24)) (1.1.0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->-r requirements.txt (line 25)) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->-r requirements.txt (line 25)) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->-r requirements.txt (line 25))\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (1.19.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (1.0.8)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 24)) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->-r requirements.txt (line 24)) (4.3.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 25)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 25)) (2.18.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->-r requirements.txt (line 24)) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->-r requirements.txt (line 24)) (1.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r requirements.txt (line 24)) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 25)) (0.1.2)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.25.0-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=8ac855a1eb49bba8c4f1fba84e688d6a113a97c1c27909bbe832bd2870c8fbf3\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: pydub, xxhash, uvicorn, tomlkit, shtab, semantic-version, ruff, python-multipart, pydantic-core, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, groovy, fsspec, fire, ffmpy, dill, av, aiofiles, tiktoken, starlette, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tyro, sse-starlette, safehttpx, nvidia-cusolver-cu12, gradio-client, fastapi, gradio, datasets, trl\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.1\n",
            "    Uninstalling pydantic_core-2.33.1:\n",
            "      Successfully uninstalled pydantic_core-2.33.1\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.3\n",
            "    Uninstalling pydantic-2.11.3:\n",
            "      Successfully uninstalled pydantic-2.11.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-24.1.0 av-14.3.0 datasets-3.5.0 dill-0.3.8 fastapi-0.115.12 ffmpy-0.5.0 fire-0.7.0 fsspec-2024.12.0 gradio-5.25.0 gradio-client-1.8.0 groovy-0.1.2 multiprocess-0.70.16 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydantic-2.10.6 pydantic-core-2.27.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.6 safehttpx-0.1.6 semantic-version-2.10.0 shtab-1.7.2 sse-starlette-2.2.1 starlette-0.46.2 tiktoken-0.9.0 tomlkit-0.13.2 trl-0.9.6 tyro-0.8.14 uvicorn-0.34.2 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj9kcyYwtUlQ",
        "outputId": "1a31f202-08c1-4102-9475-44d9ad575a96"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e \".[torch, metrics]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6b7jL1ttXF6",
        "outputId": "e1f6b2a4-87ae-4f8b-c370-adbcc5d6622e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (4.51.3)\n",
            "Requirement already satisfied: datasets<=3.5.0,>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.5.0)\n",
            "Requirement already satisfied: accelerate<=1.6.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.5.2)\n",
            "Requirement already satisfied: peft<=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.14.0)\n",
            "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.9.6)\n",
            "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.21.1)\n",
            "Requirement already satisfied: gradio<=5.25.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.25.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.14.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.29.4)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.34.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.115.12)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.10.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\n",
            "Requirement already satisfied: pydantic<=2.10.6 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.10.6)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (14.3.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.11.0)\n",
            "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.14)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.6.0+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.9.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.42.1)\n",
            "Collecting rouge-chinese (from llamafactory==0.9.3.dev0)\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.15)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.16)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.1.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.11.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.13.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (2.27.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.3.dev0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->llamafactory==0.9.3.dev0) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (1.7.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.3.dev0) (3.0.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge-chinese->llamafactory==0.9.3.dev0) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.19.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.8)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.18.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
            "Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26548 sha256=3b2b4b5aa1618c1ccb636e0e1e7054a6c55c605a6b69e20972b0f7a3ee14bad2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-syit1xu2/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: rouge-chinese, llamafactory\n",
            "Successfully installed llamafactory-0.9.3.dev0 rouge-chinese-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install liger-kernel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQxr1z0VtZXM",
        "outputId": "28cdcbe6-5bb6-4504-a22e-974477b6da3a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting liger-kernel\n",
            "  Downloading liger_kernel-0.5.8-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: torch>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from liger-kernel) (2.6.0+cu124)\n",
            "Requirement already satisfied: triton>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from liger-kernel) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.2->liger-kernel) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.2->liger-kernel) (3.0.2)\n",
            "Downloading liger_kernel-0.5.8-py3-none-any.whl (150 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/150.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: liger-kernel\n",
            "Successfully installed liger-kernel-0.5.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                        # do supervised fine-tuning\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"Qwen/Qwen2-VL-2B-Instruct\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  dataset=\"mllm_demo\",             # use alpaca and identity datasets\n",
        "  template=\"qwen2_vl\",                     # use llama3 prompt template\n",
        "  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
        "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
        "  output_dir=\"saves_1/qwen2_vl-2b_1/lora/sft\",                  # the path to save LoRA adapters\n",
        "  per_device_train_batch_size=2,               # the batch size\n",
        "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
        "  logging_steps=10,                      # log every 10 steps\n",
        "  warmup_ratio=0.1,                      # use warmup scheduler\n",
        "  save_steps=1000,                      # save checkpoint every 1000 steps\n",
        "  learning_rate=5e-5,                     # the learning rate\n",
        "  num_train_epochs=100.0,                    # the epochs of training\n",
        "  max_samples=500,                      # use 500 examples in each dataset\n",
        "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
        "  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
        "  fp16=True,                         # use float16 mixed precision training\n",
        "  use_liger_kernel=False,                   # use liger kernel for efficient training\n",
        ")"
      ],
      "metadata": {
        "id": "jiKmBaNdvfb9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json.dump(args, open(\"train_qwen2vl_2.json\", \"w\", encoding=\"utf-8\"), indent=2)"
      ],
      "metadata": {
        "id": "uezDWCNgvf_6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "VAVwjvE8vhu6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should return True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9k7c0kkvjSd",
        "outputId": "63e3f162-85c2-437b-e629-29b8a6bc57ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!llamafactory-cli train train_qwen2vl_2.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quDPx9-pvnV4",
        "outputId": "f802d264-1099-420e-df60-37dd305b2712"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-21 14:34:44.997262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745246085.017731    5246 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745246085.023996    5246 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-21 14:34:45.044483: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "[INFO|2025-04-21 14:34:52] llamafactory.hparams.parser:388 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 14:34:52,906 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 14:34:52,906 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 14:34:52,906 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 14:34:52,906 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 14:34:52,906 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 14:34:52,906 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 14:34:52,906 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-21 14:34:53,319 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|image_processing_base.py:380] 2025-04-21 14:34:53,558 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/preprocessor_config.json\n",
            "[INFO|image_processing_base.py:380] 2025-04-21 14:34:53,640 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/preprocessor_config.json\n",
            "[WARNING|logging.py:328] 2025-04-21 14:34:53,640 >> Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "[INFO|image_processing_base.py:433] 2025-04-21 14:34:53,641 >> Image processor Qwen2VLImageProcessor {\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"max_pixels\": 12845056,\n",
            "  \"merge_size\": 2,\n",
            "  \"min_pixels\": 3136,\n",
            "  \"patch_size\": 14,\n",
            "  \"processor_class\": \"Qwen2VLProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"longest_edge\": 12845056,\n",
            "    \"shortest_edge\": 3136\n",
            "  },\n",
            "  \"temporal_patch_size\": 2\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 14:34:53,725 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 14:34:53,725 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 14:34:53,725 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 14:34:53,725 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 14:34:53,725 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 14:34:53,725 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 14:34:53,725 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-21 14:34:54,099 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|processing_utils.py:884] 2025-04-21 14:34:54,985 >> Processor Qwen2VLProcessor:\n",
            "- image_processor: Qwen2VLImageProcessor {\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"max_pixels\": 12845056,\n",
            "  \"merge_size\": 2,\n",
            "  \"min_pixels\": 3136,\n",
            "  \"patch_size\": 14,\n",
            "  \"processor_class\": \"Qwen2VLProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"longest_edge\": 12845056,\n",
            "    \"shortest_edge\": 3136\n",
            "  },\n",
            "  \"temporal_patch_size\": 2\n",
            "}\n",
            "\n",
            "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2-VL-2B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            ")\n",
            "\n",
            "{\n",
            "  \"processor_class\": \"Qwen2VLProcessor\"\n",
            "}\n",
            "\n",
            "[INFO|2025-04-21 14:34:55] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n",
            "[INFO|2025-04-21 14:34:55] llamafactory.data.loader:143 >> Loading dataset mllm_demo.json...\n",
            "Generating train split: 75 examples [00:00, 6601.18 examples/s]\n",
            "Converting format of dataset:   0% 0/75 [00:00<?, ? examples/s][WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/1.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/2.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/3.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/4.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/5.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/6.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/7.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/8.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/9.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/10.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/11.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/12.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/13.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/14.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/15.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/16.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/17.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/18.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/19.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/20.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_1.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_2.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_3.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_4.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_5.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_6.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_7.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_8.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_9.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_10.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_11.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_12.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_13.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_14.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_15.webp does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_16.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_17.webp does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_18.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_19.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/cat_20.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_1.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_2.jpeg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_3.webp does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_4.png does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_5.webp does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_6.jpeg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_8.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_9.webp does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_11.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_12.webp does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_13.png does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_14.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_16.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_17.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_19.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/dog_20.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_1.webp does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_2.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_3.webp does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_4.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_5.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_7.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_8.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_9.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_10.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_11.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_12.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_13.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_14.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_15.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_16.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_17.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_18.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_19.jpg does not exist in `media_dir`. Use original path.\n",
            "[WARNING|2025-04-21 14:34:55] llamafactory.data.converter:154 >> Media kaushika/human_20.jpg does not exist in `media_dir`. Use original path.\n",
            "Converting format of dataset: 100% 75/75 [00:00<00:00, 3739.88 examples/s]\n",
            "Running tokenizer on dataset: 100% 75/75 [00:02<00:00, 32.84 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 3555, 374, 419, 30, 151645, 198, 151644, 77091, 198, 1986, 374, 458, 9864, 481, 45198, 6241, 13, 151645, 198]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|> What is this?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "This is an animal - gorilla.<|im_end|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1986, 374, 458, 9864, 481, 45198, 6241, 13, 151645, 198]\n",
            "labels:\n",
            "This is an animal - gorilla.<|im_end|>\n",
            "\n",
            "config.json: 100% 1.20k/1.20k [00:00<00:00, 8.72MB/s]\n",
            "[INFO|configuration_utils.py:693] 2025-04-21 14:34:57,889 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-21 14:34:57,896 >> Model config Qwen2VLConfig {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2_vl\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"depth\": 32,\n",
            "    \"embed_dim\": 1280,\n",
            "    \"hidden_act\": \"quick_gelu\",\n",
            "    \"hidden_size\": 1536,\n",
            "    \"in_channels\": 3,\n",
            "    \"in_chans\": 3,\n",
            "    \"mlp_ratio\": 4,\n",
            "    \"model_type\": \"qwen2_vl\",\n",
            "    \"num_heads\": 16,\n",
            "    \"patch_size\": 14,\n",
            "    \"spatial_merge_size\": 2,\n",
            "    \"spatial_patch_size\": 14,\n",
            "    \"temporal_patch_size\": 2\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|2025-04-21 14:34:57] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "model.safetensors.index.json: 100% 56.4k/56.4k [00:00<00:00, 9.71MB/s]\n",
            "[INFO|modeling_utils.py:1124] 2025-04-21 14:34:59,148 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/model.safetensors.index.json\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/3.99G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 0.00/429M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/3.99G [00:00<00:39, 101MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   5% 21.0M/429M [00:00<00:03, 129MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 31.5M/3.99G [00:00<00:24, 159MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  12% 52.4M/429M [00:00<00:02, 183MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 62.9M/3.99G [00:00<00:19, 203MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  17% 73.4M/429M [00:00<00:01, 185MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/3.99G [00:00<00:21, 180MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  22% 94.4M/429M [00:00<00:01, 175MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 105M/3.99G [00:00<00:21, 177MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  27% 115M/429M [00:00<00:01, 179MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/3.99G [00:00<00:22, 173MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  32% 136M/429M [00:00<00:01, 182MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 157M/3.99G [00:05<04:21, 14.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  37% 157M/429M [00:06<00:25, 10.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 178M/3.99G [00:06<04:09, 15.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  44% 189M/429M [00:06<00:14, 17.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 210M/3.99G [00:07<02:40, 23.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  49% 210M/429M [00:07<00:09, 23.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 231M/3.99G [00:07<02:03, 30.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  54% 231M/429M [00:07<00:06, 30.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 252M/3.99G [00:07<01:34, 39.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  59% 252M/429M [00:07<00:04, 39.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 273M/3.99G [00:07<01:14, 50.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  63% 273M/429M [00:07<00:03, 51.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 294M/3.99G [00:07<00:58, 63.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  68% 294M/429M [00:07<00:02, 63.3MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 315M/3.99G [00:07<00:50, 72.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  73% 315M/429M [00:07<00:01, 77.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 336M/3.99G [00:07<00:42, 85.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  78% 336M/429M [00:07<00:01, 90.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  83% 357M/429M [00:07<00:00, 101MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 357M/3.99G [00:08<00:39, 90.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  88% 377M/429M [00:08<00:00, 107MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 377M/3.99G [00:08<00:37, 96.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  93% 398M/429M [00:08<00:00, 119MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 398M/3.99G [00:08<00:33, 107MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  98% 419M/429M [00:08<00:00, 124MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 429M/429M [00:08<00:00, 50.2MB/s]\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 440M/3.99G [00:08<00:27, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 461M/3.99G [00:08<00:25, 140MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 493M/3.99G [00:08<00:21, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 514M/3.99G [00:08<00:20, 171MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 535M/3.99G [00:09<00:20, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 556M/3.99G [00:09<00:20, 170MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 587M/3.99G [00:09<00:17, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 619M/3.99G [00:09<00:15, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 650M/3.99G [00:09<00:14, 228MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 682M/3.99G [00:09<00:13, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 713M/3.99G [00:09<00:13, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 744M/3.99G [00:09<00:12, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 776M/3.99G [00:10<00:12, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 807M/3.99G [00:10<00:19, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 839M/3.99G [00:10<00:16, 188MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 870M/3.99G [00:10<00:15, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 902M/3.99G [00:10<00:14, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 933M/3.99G [00:10<00:13, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 965M/3.99G [00:11<00:12, 235MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 996M/3.99G [00:11<00:12, 238MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.03G/3.99G [00:11<00:12, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.06G/3.99G [00:11<00:12, 241MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.09G/3.99G [00:11<00:11, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.12G/3.99G [00:11<00:11, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.15G/3.99G [00:11<00:11, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.18G/3.99G [00:11<00:11, 243MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.22G/3.99G [00:12<00:11, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.25G/3.99G [00:12<00:10, 262MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.28G/3.99G [00:12<00:10, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.31G/3.99G [00:12<00:11, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.34G/3.99G [00:12<00:10, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.37G/3.99G [00:12<00:10, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.41G/3.99G [00:12<00:10, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.44G/3.99G [00:12<00:09, 260MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.47G/3.99G [00:13<00:09, 261MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.50G/3.99G [00:13<00:09, 263MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.53G/3.99G [00:13<00:09, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.56G/3.99G [00:13<00:09, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.59G/3.99G [00:13<00:09, 260MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 1.63G/3.99G [00:13<00:09, 261MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 1.66G/3.99G [00:13<00:08, 261MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 1.69G/3.99G [00:13<00:08, 260MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 1.72G/3.99G [00:13<00:08, 262MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 1.75G/3.99G [00:14<00:08, 261MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 1.78G/3.99G [00:14<00:08, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 1.81G/3.99G [00:14<00:08, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 1.85G/3.99G [00:14<00:08, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 1.88G/3.99G [00:14<00:08, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 1.91G/3.99G [00:14<00:08, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 1.94G/3.99G [00:14<00:08, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 1.97G/3.99G [00:15<00:08, 242MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.00G/3.99G [00:15<00:08, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.03G/3.99G [00:15<00:08, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.07G/3.99G [00:15<00:08, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.10G/3.99G [00:15<00:08, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.13G/3.99G [00:15<00:08, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.16G/3.99G [00:15<00:08, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.19G/3.99G [00:16<00:08, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.22G/3.99G [00:16<00:08, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.25G/3.99G [00:16<00:07, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.29G/3.99G [00:16<00:07, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.32G/3.99G [00:16<00:07, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.35G/3.99G [00:16<00:07, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.38G/3.99G [00:16<00:07, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.41G/3.99G [00:17<00:07, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 2.44G/3.99G [00:17<00:06, 228MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 2.47G/3.99G [00:17<00:06, 236MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 2.51G/3.99G [00:17<00:06, 238MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 2.54G/3.99G [00:17<00:06, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 2.57G/3.99G [00:17<00:06, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 2.60G/3.99G [00:17<00:06, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 2.63G/3.99G [00:18<00:05, 229MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 2.66G/3.99G [00:18<00:05, 236MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 2.69G/3.99G [00:18<00:05, 241MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 2.73G/3.99G [00:18<00:05, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 2.76G/3.99G [00:18<00:04, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 2.79G/3.99G [00:18<00:04, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 2.82G/3.99G [00:18<00:04, 257MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 2.85G/3.99G [00:18<00:04, 253MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 2.88G/3.99G [00:18<00:04, 257MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 2.92G/3.99G [00:19<00:04, 258MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 2.95G/3.99G [00:19<00:04, 259MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 2.98G/3.99G [00:19<00:03, 257MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.01G/3.99G [00:19<00:03, 258MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.04G/3.99G [00:19<00:03, 257MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.07G/3.99G [00:19<00:03, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.10G/3.99G [00:19<00:03, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.14G/3.99G [00:19<00:03, 266MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.17G/3.99G [00:20<00:03, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.20G/3.99G [00:20<00:03, 263MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 3.23G/3.99G [00:20<00:03, 236MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 3.26G/3.99G [00:20<00:03, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 3.29G/3.99G [00:21<00:05, 119MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 3.31G/3.99G [00:21<00:05, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 3.33G/3.99G [00:21<00:04, 142MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 3.36G/3.99G [00:21<00:04, 154MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 3.38G/3.99G [00:21<00:03, 162MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 3.41G/3.99G [00:21<00:03, 176MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 3.43G/3.99G [00:21<00:03, 183MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 3.46G/3.99G [00:21<00:02, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 3.49G/3.99G [00:22<00:02, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 3.52G/3.99G [00:25<00:17, 27.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 3.54G/3.99G [00:25<00:13, 33.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 3.57G/3.99G [00:25<00:09, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 3.60G/3.99G [00:25<00:06, 60.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 3.63G/3.99G [00:25<00:04, 81.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 3.66G/3.99G [00:25<00:03, 103MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 3.69G/3.99G [00:26<00:02, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 3.72G/3.99G [00:26<00:01, 148MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 3.75G/3.99G [00:26<00:01, 168MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 3.79G/3.99G [00:26<00:01, 187MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 3.82G/3.99G [00:26<00:00, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 3.85G/3.99G [00:26<00:00, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 3.88G/3.99G [00:26<00:00, 231MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 3.91G/3.99G [00:26<00:00, 243MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 3.94G/3.99G [00:26<00:00, 248MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 3.99G/3.99G [00:27<00:00, 147MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:27<00:00, 13.68s/it]\n",
            "[INFO|modeling_utils.py:2167] 2025-04-21 14:35:26,608 >> Instantiating Qwen2VLForConditionalGeneration model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1142] 2025-04-21 14:35:26,609 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2167] 2025-04-21 14:35:26,610 >> Instantiating Qwen2VisionTransformerPretrainedModel model under default dtype torch.float16.\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.08s/it]\n",
            "[INFO|modeling_utils.py:4930] 2025-04-21 14:35:28,967 >> All model checkpoint weights were used when initializing Qwen2VLForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4938] 2025-04-21 14:35:28,967 >> All the weights of Qwen2VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2VLForConditionalGeneration for predictions without further training.\n",
            "generation_config.json: 100% 272/272 [00:00<00:00, 1.61MB/s]\n",
            "[INFO|configuration_utils.py:1097] 2025-04-21 14:35:29,144 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/generation_config.json\n",
            "[INFO|configuration_utils.py:1142] 2025-04-21 14:35:29,145 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"temperature\": 0.01,\n",
            "  \"top_k\": 1,\n",
            "  \"top_p\": 0.001\n",
            "}\n",
            "\n",
            "[INFO|2025-04-21 14:35:29] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-04-21 14:35:29] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-04-21 14:35:29] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-04-21 14:35:29] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-04-21 14:35:29] llamafactory.model.model_utils.misc:143 >> Found linear modules: gate_proj,k_proj,up_proj,q_proj,down_proj,o_proj,v_proj\n",
            "[INFO|2025-04-21 14:35:29] llamafactory.model.model_utils.visual:143 >> Set vision model not trainable: ['visual.patch_embed', 'visual.blocks'].\n",
            "[INFO|2025-04-21 14:35:29] llamafactory.model.model_utils.visual:143 >> Set multi model projector not trainable: visual.merger.\n",
            "[INFO|2025-04-21 14:35:31] llamafactory.model.loader:143 >> trainable params: 9,232,384 || all params: 2,218,217,984 || trainable%: 0.4162\n",
            "[INFO|trainer.py:748] 2025-04-21 14:35:31,220 >> Using auto half precision backend\n",
            "[WARNING|trainer.py:783] 2025-04-21 14:35:31,221 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "[INFO|2025-04-21 14:35:31] llamafactory.train.trainer_utils:143 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
            "[INFO|trainer.py:2414] 2025-04-21 14:35:31,680 >> ***** Running training *****\n",
            "[INFO|trainer.py:2415] 2025-04-21 14:35:31,680 >>   Num examples = 75\n",
            "[INFO|trainer.py:2416] 2025-04-21 14:35:31,680 >>   Num Epochs = 100\n",
            "[INFO|trainer.py:2417] 2025-04-21 14:35:31,680 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2420] 2025-04-21 14:35:31,680 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2421] 2025-04-21 14:35:31,680 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2422] 2025-04-21 14:35:31,680 >>   Total optimization steps = 900\n",
            "[INFO|trainer.py:2423] 2025-04-21 14:35:31,688 >>   Number of trainable parameters = 9,232,384\n",
            "{'loss': 2.339, 'grad_norm': 1.6650519371032715, 'learning_rate': 5e-06, 'epoch': 1.0}\n",
            "{'loss': 0.8932, 'grad_norm': 0.4162410497665405, 'learning_rate': 1.0555555555555555e-05, 'epoch': 2.0}\n",
            "{'loss': 0.0846, 'grad_norm': 0.005783152766525745, 'learning_rate': 1.6111111111111115e-05, 'epoch': 3.0}\n",
            "{'loss': 0.0003, 'grad_norm': 0.0034102057106792927, 'learning_rate': 2.1666666666666667e-05, 'epoch': 4.0}\n",
            "{'loss': 0.0002, 'grad_norm': 0.001204196596518159, 'learning_rate': 2.7222222222222223e-05, 'epoch': 5.0}\n",
            "{'loss': 0.0001, 'grad_norm': 0.0007348215440288186, 'learning_rate': 3.277777777777778e-05, 'epoch': 6.0}\n",
            "{'loss': 0.0, 'grad_norm': 0.0003548625682014972, 'learning_rate': 3.8333333333333334e-05, 'epoch': 7.0}\n",
            "{'loss': 0.0, 'grad_norm': 0.00013973860768601298, 'learning_rate': 4.388888888888889e-05, 'epoch': 8.0}\n",
            "{'loss': 0.0, 'grad_norm': 0.0006472795503214002, 'learning_rate': 4.9444444444444446e-05, 'epoch': 9.0}\n",
            "{'loss': 0.0, 'grad_norm': 0.00019029126269742846, 'learning_rate': 4.99847706754774e-05, 'epoch': 10.0}\n",
            "{'loss': 0.0, 'grad_norm': 0.0001470270217396319, 'learning_rate': 4.993214991772563e-05, 'epoch': 11.0}\n",
            "{'loss': 0.0, 'grad_norm': 0.00012683362001553178, 'learning_rate': 4.9842028838154285e-05, 'epoch': 12.0}\n",
            "{'loss': 0.0, 'grad_norm': 6.929250957909971e-05, 'learning_rate': 4.971454298742779e-05, 'epoch': 13.0}\n",
            "{'loss': 0.0, 'grad_norm': 0.0001794432901078835, 'learning_rate': 4.9549884116375714e-05, 'epoch': 14.0}\n",
            "{'loss': 0.0, 'grad_norm': 0.00027971266536042094, 'learning_rate': 4.934829988758131e-05, 'epoch': 15.0}\n",
            "{'loss': 0.0, 'grad_norm': 7.078397902660072e-05, 'learning_rate': 4.9110093502873476e-05, 'epoch': 16.0}\n",
            "{'loss': 0.0, 'grad_norm': 0.00012117072765249759, 'learning_rate': 4.883562324728241e-05, 'epoch': 17.0}\n",
            "{'loss': 0.0, 'grad_norm': 5.380398943088949e-05, 'learning_rate': 4.8525301950144894e-05, 'epoch': 18.0}\n",
            "{'loss': 0.0, 'grad_norm': 0.00012296515342313796, 'learning_rate': 4.817959636416969e-05, 'epoch': 19.0}\n",
            "{'loss': 0.0, 'grad_norm': 9.90718326647766e-05, 'learning_rate': 4.779902646339722e-05, 'epoch': 20.0}\n",
            "{'loss': 0.0, 'grad_norm': 9.513188706478104e-05, 'learning_rate': 4.7384164661109176e-05, 'epoch': 21.0}\n",
            "{'loss': 0.0, 'grad_norm': 0.00010876554006244987, 'learning_rate': 4.693563494886455e-05, 'epoch': 22.0}\n",
            "{'loss': 0.0, 'grad_norm': 5.91421194258146e-05, 'learning_rate': 4.645411195795709e-05, 'epoch': 23.0}\n",
            "{'loss': 0.0, 'grad_norm': 4.556784188025631e-05, 'learning_rate': 4.5940319944705736e-05, 'epoch': 24.0}\n",
            "{'loss': 0.0, 'grad_norm': 5.367694029700942e-05, 'learning_rate': 4.539503170110431e-05, 'epoch': 25.0}\n",
            "{'loss': 0.0, 'grad_norm': 4.727185660158284e-05, 'learning_rate': 4.4819067392468944e-05, 'epoch': 26.0}\n",
            "{'loss': 0.0, 'grad_norm': 8.036880899453536e-05, 'learning_rate': 4.4213293323831585e-05, 'epoch': 27.0}\n",
            "{'loss': 0.0, 'grad_norm': 5.213284384808503e-05, 'learning_rate': 4.357862063693486e-05, 'epoch': 28.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.984847560583148e-05, 'learning_rate': 4.2916003939788403e-05, 'epoch': 29.0}\n",
            "{'loss': 0.0, 'grad_norm': 4.1566203435650095e-05, 'learning_rate': 4.22264398708477e-05, 'epoch': 30.0}\n",
            "{'loss': 0.0, 'grad_norm': 4.2784293327713385e-05, 'learning_rate': 4.1510965599975196e-05, 'epoch': 31.0}\n",
            "{'loss': 0.0, 'grad_norm': 3.941558316000737e-05, 'learning_rate': 4.077065726843828e-05, 'epoch': 32.0}\n",
            "{'loss': 0.0, 'grad_norm': 3.182108048349619e-05, 'learning_rate': 4.000662837029062e-05, 'epoch': 33.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.5262936105718836e-05, 'learning_rate': 3.9220028077571295e-05, 'epoch': 34.0}\n",
            "{'loss': 0.0, 'grad_norm': 3.189219205523841e-05, 'learning_rate': 3.841203951184095e-05, 'epoch': 35.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.65751968981931e-05, 'learning_rate': 3.75838779646545e-05, 'epoch': 36.0}\n",
            "{'loss': 0.0, 'grad_norm': 3.1198287615552545e-05, 'learning_rate': 3.673678906964727e-05, 'epoch': 37.0}\n",
            "{'loss': 0.0, 'grad_norm': 3.34102260239888e-05, 'learning_rate': 3.5872046928983626e-05, 'epoch': 38.0}\n",
            "{'loss': 0.0, 'grad_norm': 4.7630885092075914e-05, 'learning_rate': 3.499095219698631e-05, 'epoch': 39.0}\n",
            "{'loss': 0.0, 'grad_norm': 0.0001367170480079949, 'learning_rate': 3.409483012382879e-05, 'epoch': 40.0}\n",
            "{'loss': 0.0, 'grad_norm': 3.251554517191835e-05, 'learning_rate': 3.318502856223311e-05, 'epoch': 41.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.696354204090312e-05, 'learning_rate': 3.2262915940171376e-05, 'epoch': 42.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.150944601453375e-05, 'learning_rate': 3.132987920262005e-05, 'epoch': 43.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.9101928046438843e-05, 'learning_rate': 3.0387321725463e-05, 'epoch': 44.0}\n",
            "{'loss': 0.0, 'grad_norm': 3.660329457488842e-05, 'learning_rate': 2.9436661204680882e-05, 'epoch': 45.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.854534275480546e-05, 'learning_rate': 2.8479327524001636e-05, 'epoch': 46.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.621383646328468e-05, 'learning_rate': 2.7516760604219617e-05, 'epoch': 47.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.588477607583627e-05, 'learning_rate': 2.6550408237417885e-05, 'epoch': 48.0}\n",
            "{'loss': 0.0, 'grad_norm': 4.045140303787775e-05, 'learning_rate': 2.5581723909351406e-05, 'epoch': 49.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.5185136109939776e-05, 'learning_rate': 2.461216461326642e-05, 'epoch': 50.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.9157879907870665e-05, 'learning_rate': 2.364318865844416e-05, 'epoch': 51.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.844169855758082e-05, 'learning_rate': 2.2676253476765196e-05, 'epoch': 52.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.7457656213082373e-05, 'learning_rate': 2.1712813430593436e-05, 'epoch': 53.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.3627824702998623e-05, 'learning_rate': 2.0754317625276983e-05, 'epoch': 54.0}\n",
            "{'loss': 0.0, 'grad_norm': 5.2991843404015526e-05, 'learning_rate': 1.980220772955602e-05, 'epoch': 55.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.273729296575766e-05, 'learning_rate': 1.8857915807156092e-05, 'epoch': 56.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.8907938283518888e-05, 'learning_rate': 1.792286216282824e-05, 'epoch': 57.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.3838141967426054e-05, 'learning_rate': 1.699845320607571e-05, 'epoch': 58.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.6697766113793477e-05, 'learning_rate': 1.60860793357805e-05, 'epoch': 59.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.124664024449885e-05, 'learning_rate': 1.5187112848911323e-05, 'epoch': 60.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.9366478227311745e-05, 'learning_rate': 1.430290587645865e-05, 'epoch': 61.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.171587766497396e-05, 'learning_rate': 1.343478834970121e-05, 'epoch': 62.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.0124276488786563e-05, 'learning_rate': 1.2584065999863102e-05, 'epoch': 63.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.5423767510801554e-05, 'learning_rate': 1.175201839416988e-05, 'epoch': 64.0}\n",
            "{'loss': 0.0, 'grad_norm': 3.235589247196913e-05, 'learning_rate': 1.0939897011258001e-05, 'epoch': 65.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.074499207083136e-05, 'learning_rate': 1.0148923358832022e-05, 'epoch': 66.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.4777931028220337e-05, 'learning_rate': 9.380287136401e-06, 'epoch': 67.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.438337105559185e-05, 'learning_rate': 8.635144445857406e-06, 'epoch': 68.0}\n",
            "{'loss': 0.0, 'grad_norm': 5.3040530474390835e-05, 'learning_rate': 7.914616052590071e-06, 'epoch': 69.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.4019795091589913e-05, 'learning_rate': 7.219785699746573e-06, 'epoch': 70.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.9593131582951173e-05, 'learning_rate': 6.55169847818059e-06, 'epoch': 71.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.861090277088806e-05, 'learning_rate': 5.9113592545359945e-06, 'epoch': 72.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.709153366391547e-05, 'learning_rate': 5.299731159831953e-06, 'epoch': 73.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.1506006305571645e-05, 'learning_rate': 4.7177341408224e-06, 'epoch': 74.0}\n",
            "{'loss': 0.0, 'grad_norm': 4.429417094797827e-05, 'learning_rate': 4.166243576308712e-06, 'epoch': 75.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.084198422380723e-05, 'learning_rate': 3.6460889604868626e-06, 'epoch': 76.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.6535299437236972e-05, 'learning_rate': 3.158052655309332e-06, 'epoch': 77.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.6975312721333466e-05, 'learning_rate': 2.7028687137384267e-06, 'epoch': 78.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.900435017887503e-05, 'learning_rate': 2.281221775660894e-06, 'epoch': 79.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.666317621129565e-05, 'learning_rate': 1.893746038124497e-06, 'epoch': 80.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.4322355127660558e-05, 'learning_rate': 1.541024301445404e-06, 'epoch': 81.0}\n",
            "{'loss': 0.0, 'grad_norm': 5.51317680219654e-05, 'learning_rate': 1.2235870926211619e-06, 'epoch': 82.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.9431367036304437e-05, 'learning_rate': 9.419118673676924e-07, 'epoch': 83.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.5368561435025185e-05, 'learning_rate': 6.964222919805391e-07, 'epoch': 84.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.9217955923522823e-05, 'learning_rate': 4.874876061005173e-07, 'epoch': 85.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.8645430827746168e-05, 'learning_rate': 3.1542206734221924e-07, 'epoch': 86.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.711915319901891e-05, 'learning_rate': 1.8048447862070718e-07, 'epoch': 87.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.697181141935289e-05, 'learning_rate': 8.28777988873486e-08, 'epoch': 88.0}\n",
            "{'loss': 0.0, 'grad_norm': 1.6105985196190886e-05, 'learning_rate': 2.2748837860270267e-08, 'epoch': 89.0}\n",
            "{'loss': 0.0, 'grad_norm': 2.1743931938544847e-05, 'learning_rate': 1.8803520859811406e-10, 'epoch': 90.0}\n",
            "100% 900/900 [1:04:47<00:00,  3.92s/it][INFO|trainer.py:3984] 2025-04-21 15:40:19,642 >> Saving model checkpoint to saves_1/qwen2_vl-2b_1/lora/sft/checkpoint-900\n",
            "[INFO|configuration_utils.py:693] 2025-04-21 15:40:19,856 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-21 15:40:19,858 >> Model config Qwen2VLConfig {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2_vl\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"depth\": 32,\n",
            "    \"embed_dim\": 1280,\n",
            "    \"hidden_act\": \"quick_gelu\",\n",
            "    \"hidden_size\": 1536,\n",
            "    \"in_channels\": 3,\n",
            "    \"in_chans\": 3,\n",
            "    \"mlp_ratio\": 4,\n",
            "    \"model_type\": \"qwen2_vl\",\n",
            "    \"num_heads\": 16,\n",
            "    \"patch_size\": 14,\n",
            "    \"spatial_merge_size\": 2,\n",
            "    \"spatial_patch_size\": 14,\n",
            "    \"temporal_patch_size\": 2\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-21 15:40:19,948 >> tokenizer config file saved in saves_1/qwen2_vl-2b_1/lora/sft/checkpoint-900/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-21 15:40:19,949 >> Special tokens file saved in saves_1/qwen2_vl-2b_1/lora/sft/checkpoint-900/special_tokens_map.json\n",
            "[INFO|image_processing_base.py:260] 2025-04-21 15:40:20,455 >> Image processor saved in saves_1/qwen2_vl-2b_1/lora/sft/checkpoint-900/preprocessor_config.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-21 15:40:20,465 >> tokenizer config file saved in saves_1/qwen2_vl-2b_1/lora/sft/checkpoint-900/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-21 15:40:20,466 >> Special tokens file saved in saves_1/qwen2_vl-2b_1/lora/sft/checkpoint-900/special_tokens_map.json\n",
            "[INFO|processing_utils.py:648] 2025-04-21 15:40:21,557 >> chat template saved in saves_1/qwen2_vl-2b_1/lora/sft/checkpoint-900/chat_template.json\n",
            "[INFO|trainer.py:2681] 2025-04-21 15:40:21,557 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3889.8693, 'train_samples_per_second': 1.928, 'train_steps_per_second': 0.231, 'train_loss': 0.03686562489092264, 'epoch': 90.0}\n",
            "100% 900/900 [1:04:49<00:00,  4.32s/it]\n",
            "[INFO|image_processing_base.py:260] 2025-04-21 15:40:21,561 >> Image processor saved in saves_1/qwen2_vl-2b_1/lora/sft/preprocessor_config.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-21 15:40:21,562 >> tokenizer config file saved in saves_1/qwen2_vl-2b_1/lora/sft/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-21 15:40:21,562 >> Special tokens file saved in saves_1/qwen2_vl-2b_1/lora/sft/special_tokens_map.json\n",
            "[INFO|processing_utils.py:648] 2025-04-21 15:40:22,510 >> chat template saved in saves_1/qwen2_vl-2b_1/lora/sft/chat_template.json\n",
            "[INFO|trainer.py:3984] 2025-04-21 15:40:22,510 >> Saving model checkpoint to saves_1/qwen2_vl-2b_1/lora/sft\n",
            "[INFO|configuration_utils.py:693] 2025-04-21 15:40:22,778 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-21 15:40:22,779 >> Model config Qwen2VLConfig {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2_vl\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"depth\": 32,\n",
            "    \"embed_dim\": 1280,\n",
            "    \"hidden_act\": \"quick_gelu\",\n",
            "    \"hidden_size\": 1536,\n",
            "    \"in_channels\": 3,\n",
            "    \"in_chans\": 3,\n",
            "    \"mlp_ratio\": 4,\n",
            "    \"model_type\": \"qwen2_vl\",\n",
            "    \"num_heads\": 16,\n",
            "    \"patch_size\": 14,\n",
            "    \"spatial_merge_size\": 2,\n",
            "    \"spatial_patch_size\": 14,\n",
            "    \"temporal_patch_size\": 2\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-21 15:40:22,879 >> tokenizer config file saved in saves_1/qwen2_vl-2b_1/lora/sft/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-21 15:40:22,879 >> Special tokens file saved in saves_1/qwen2_vl-2b_1/lora/sft/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       90.0\n",
            "  total_flos               = 31287957GF\n",
            "  train_loss               =     0.0369\n",
            "  train_runtime            = 1:04:49.86\n",
            "  train_samples_per_second =      1.928\n",
            "  train_steps_per_second   =      0.231\n",
            "[INFO|modelcard.py:450] 2025-04-21 15:40:23,104 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"Qwen/Qwen2-VL-2B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"saves_1/qwen2_vl-2b_1/lora/sft\",            # load the saved LoRA adapters\n",
        "  template=\"qwen2_vl\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  export_dir=\"qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika\",              # the path to save the merged model\n",
        "  export_size=2,                       # the file shard size (in GB) of the merged model\n",
        "  export_device=\"cpu\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n",
        ")\n"
      ],
      "metadata": {
        "id": "shmbeYH9vpzJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "json.dump(args, open(\"merge_qwen2vl.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMOKjOmDxoyQ",
        "outputId": "15230770-831a-42d5-cb04-55e5eb243309"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!llamafactory-cli export merge_qwen2vl.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72-pIhbuxuCf",
        "outputId": "5d54741e-cf84-4188-d9cd-90ff6747b611"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-21 15:40:43.935489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745250043.956284   21928 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745250043.962542   21928 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-21 15:40:43.985436: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 15:40:52,006 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 15:40:52,006 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 15:40:52,007 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 15:40:52,007 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 15:40:52,007 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 15:40:52,007 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 15:40:52,007 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-21 15:40:52,358 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|image_processing_base.py:380] 2025-04-21 15:40:52,612 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/preprocessor_config.json\n",
            "[INFO|image_processing_base.py:380] 2025-04-21 15:40:52,722 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/preprocessor_config.json\n",
            "[WARNING|logging.py:328] 2025-04-21 15:40:52,723 >> Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "[INFO|image_processing_base.py:433] 2025-04-21 15:40:52,724 >> Image processor Qwen2VLImageProcessor {\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"max_pixels\": 12845056,\n",
            "  \"merge_size\": 2,\n",
            "  \"min_pixels\": 3136,\n",
            "  \"patch_size\": 14,\n",
            "  \"processor_class\": \"Qwen2VLProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"longest_edge\": 12845056,\n",
            "    \"shortest_edge\": 3136\n",
            "  },\n",
            "  \"temporal_patch_size\": 2\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 15:40:52,802 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 15:40:52,802 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 15:40:52,802 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 15:40:52,802 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 15:40:52,802 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 15:40:52,802 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-21 15:40:52,802 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-21 15:40:53,162 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|processing_utils.py:884] 2025-04-21 15:40:54,033 >> Processor Qwen2VLProcessor:\n",
            "- image_processor: Qwen2VLImageProcessor {\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"max_pixels\": 12845056,\n",
            "  \"merge_size\": 2,\n",
            "  \"min_pixels\": 3136,\n",
            "  \"patch_size\": 14,\n",
            "  \"processor_class\": \"Qwen2VLProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"longest_edge\": 12845056,\n",
            "    \"shortest_edge\": 3136\n",
            "  },\n",
            "  \"temporal_patch_size\": 2\n",
            "}\n",
            "\n",
            "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2-VL-2B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            ")\n",
            "\n",
            "{\n",
            "  \"processor_class\": \"Qwen2VLProcessor\"\n",
            "}\n",
            "\n",
            "[INFO|2025-04-21 15:40:54] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n",
            "[INFO|configuration_utils.py:693] 2025-04-21 15:40:54,346 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-21 15:40:54,351 >> Model config Qwen2VLConfig {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2_vl\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"depth\": 32,\n",
            "    \"embed_dim\": 1280,\n",
            "    \"hidden_act\": \"quick_gelu\",\n",
            "    \"hidden_size\": 1536,\n",
            "    \"in_channels\": 3,\n",
            "    \"in_chans\": 3,\n",
            "    \"mlp_ratio\": 4,\n",
            "    \"model_type\": \"qwen2_vl\",\n",
            "    \"num_heads\": 16,\n",
            "    \"patch_size\": 14,\n",
            "    \"spatial_merge_size\": 2,\n",
            "    \"spatial_patch_size\": 14,\n",
            "    \"temporal_patch_size\": 2\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|2025-04-21 15:40:54] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
            "[INFO|modeling_utils.py:1124] 2025-04-21 15:40:54,371 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2167] 2025-04-21 15:40:54,372 >> Instantiating Qwen2VLForConditionalGeneration model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1142] 2025-04-21 15:40:54,375 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2167] 2025-04-21 15:40:54,375 >> Instantiating Qwen2VisionTransformerPretrainedModel model under default dtype torch.bfloat16.\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  4.11it/s]\n",
            "[INFO|modeling_utils.py:4930] 2025-04-21 15:40:54,940 >> All model checkpoint weights were used when initializing Qwen2VLForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4938] 2025-04-21 15:40:54,940 >> All the weights of Qwen2VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2VLForConditionalGeneration for predictions without further training.\n",
            "[INFO|configuration_utils.py:1097] 2025-04-21 15:40:55,029 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c/generation_config.json\n",
            "[INFO|configuration_utils.py:1142] 2025-04-21 15:40:55,030 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"temperature\": 0.01,\n",
            "  \"top_k\": 1,\n",
            "  \"top_p\": 0.001\n",
            "}\n",
            "\n",
            "[INFO|2025-04-21 15:40:55] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-04-21 15:41:18] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n",
            "[INFO|2025-04-21 15:41:18] llamafactory.model.adapter:143 >> Loaded adapter(s): saves_1/qwen2_vl-2b_1/lora/sft\n",
            "[INFO|2025-04-21 15:41:18] llamafactory.model.loader:143 >> all params: 2,208,985,600\n",
            "[INFO|2025-04-21 15:41:18] llamafactory.train.tuner:143 >> Convert model dtype to: torch.bfloat16.\n",
            "[INFO|configuration_utils.py:419] 2025-04-21 15:41:18,047 >> Configuration saved in qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika/config.json\n",
            "[INFO|configuration_utils.py:911] 2025-04-21 15:41:18,047 >> Configuration saved in qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika/generation_config.json\n",
            "[INFO|modeling_utils.py:3580] 2025-04-21 15:43:23,862 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-21 15:43:23,895 >> tokenizer config file saved in qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-21 15:43:23,896 >> Special tokens file saved in qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika/special_tokens_map.json\n",
            "[INFO|image_processing_base.py:260] 2025-04-21 15:43:24,143 >> Image processor saved in qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika/preprocessor_config.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-21 15:43:24,144 >> tokenizer config file saved in qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-21 15:43:24,145 >> Special tokens file saved in qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika/special_tokens_map.json\n",
            "[INFO|processing_utils.py:648] 2025-04-21 15:43:24,926 >> chat template saved in qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika/chat_template.json\n",
            "[INFO|2025-04-21 15:43:24] llamafactory.train.tuner:143 >> Ollama modelfile saved in qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika/Modelfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model_path = \"qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika\""
      ],
      "metadata": {
        "id": "rqPA7kZvxzhh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_zuA7MsyBVW",
        "outputId": "93d9e855-1872-474f-aa8e-67611c12e45e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, HfFolder, Repository"
      ],
      "metadata": {
        "id": "wTwuaI87yETT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api = HfApi()"
      ],
      "metadata": {
        "id": "YvhPtqj2yHQO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"-----\")  # Replace with your Hugging Face access token\n"
      ],
      "metadata": {
        "id": "ie1VHOKYyJQd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model_repo = \"Kaushika04/qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika\""
      ],
      "metadata": {
        "id": "bQcvjUS1yLE6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api.create_repo(repo_id=hf_model_repo, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "mst2n0_AyWhG",
        "outputId": "dcb4d032-9cc5-4649-9587-41f3c1d9a0b9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RepoUrl('https://huggingface.co/Kaushika04/qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika', endpoint='https://huggingface.co', repo_type='model', repo_id='Kaushika04/qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika')"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api.upload_folder(\n",
        "    folder_path=final_model_path,    # The folder containing the model files\n",
        "    repo_id=hf_model_repo,                # Your authentication token\n",
        "    commit_message=\"Initial model upload\"  # Optional commit message\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "d06c77e4d479422187ea64456ff48f19",
            "da5ee74eecc44f3a87ce70c93d18dfa1",
            "67a526f8b833440cb0d30c1c22971b4a",
            "0160bc1c7b5b4e3a8cf0544e8082e7ce",
            "2088bb46029742e8922f1e9462de823a",
            "1fed6b01294b4976b80d77fddb10798b",
            "758f29690b604857b0317504a704ee91",
            "21534abfb5154cbd96fa65e86131bc24",
            "eb056b8ac6dd4a7b98813962ef949417",
            "1dd40a498f7e4582bf150bac8d6018bc",
            "3025a95ef70b4417a7b46e725aa0f424",
            "67bd2c3658564480b0be83099abce50c",
            "f86c579ef2204b70b497e73de7d55aea",
            "f32a0004ca5c4a06b62bd953343f4709",
            "e3bd08890d0d4ec1b3792ff4ceb0952a",
            "89281cb9a8134ba8bccd4b30d986c43c",
            "b504eff049974838a7ea541059a9481a",
            "e256ac32142e4528a966636f43ae7df3",
            "e59337ce792e48ab9a72b66038c1dba6",
            "3d9eee60cf324eaa88e5137aa179a518",
            "6946380fe163449c99f1bb09e5713cf3",
            "7896d3016a834a6dbde1c78d2fafd087",
            "d3b9cc0286e04033914b8a005e2eab94",
            "6fb7f82f7e084b9d8fb1fae0af81e70b",
            "552bb27b28ae44209b187223b2262409",
            "e7478cd26fcc43a7b1d20a6d14e27f5b",
            "7038fb211bcb43e38a5cd457856aee58",
            "44a3e78a2b5247eda71b16c9521f7141",
            "9e2d0c6f64954a2c86e22632a4ae0d3f",
            "982abbe7cce749ad9d3cb733517bf75e",
            "a1fcaf8888854c269ed1406c6130ee83",
            "d15ecd97393b4feaa2bf4776e7ddd978",
            "0375cd40ba2b4e20b438bb84edac8d36",
            "08af4eff77c6477d958f46f9e1317367",
            "44101207a6d54004995e1be9b51f27cd",
            "64dad4b97a8a45689d9c6582994e822f",
            "f64697690e954d0c8c6ca74345ad5199",
            "3e6652e91969458bb31db2c26875349f",
            "a5e6802660a04195974840b376ee3759",
            "f5c09a4186324911ab5bf3b76c5855fa",
            "f96c235e1ae84048b12c1a26d0be0b13",
            "74e4c67f72964824b83d9512499f5c35",
            "400a2d4ed1a4404989894296d25e0ddb",
            "c759faeeeb6848d3a875df9421194b76",
            "0b040f4696764352b49ed8f2cdfd90af",
            "7abe6e4d8f274d26947603e494e47751",
            "f8b75ed557af4c99a719f891bd8fe5c1",
            "7edcf4d8308b4954af46a11bcd1e28cb",
            "c197e4d23a664057ae40e815ff525fef",
            "754b55f1916f42ce9c25657c6ac4ba1b",
            "a41d129f5f51412ab90fd96a23c5abca",
            "a6e857ed22a345b8b0361a31f0d89ff8",
            "78d065e7b1814c28bae7e89799ae55bf",
            "0ec01fc297ab4a5eacd898a3c0fa1ab7",
            "665d56b3329f40c084e60cc6fa56f574"
          ]
        },
        "id": "uGPEurnUyYfP",
        "outputId": "19eb4eff-6bcf-4209-e02d-7b46574ec050"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/1.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d06c77e4d479422187ea64456ff48f19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67bd2c3658564480b0be83099abce50c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3b9cc0286e04033914b8a005e2eab94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08af4eff77c6477d958f46f9e1317367"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b040f4696764352b49ed8f2cdfd90af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/Kaushika04/qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika/commit/e2bee83738dc3439e6a830ecddcf40a32c0034b8', commit_message='Initial model upload', commit_description='', oid='e2bee83738dc3439e6a830ecddcf40a32c0034b8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Kaushika04/qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika', endpoint='https://huggingface.co', repo_type='model', repo_id='Kaushika04/qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qwen-vl-utils\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bqo-1AMSyaQt",
        "outputId": "f9f3d7c4-9ea6-4bd3-d1cc-3a72e222b4e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qwen-vl-utils in /usr/local/lib/python3.11/dist-packages (0.0.11)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (14.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (24.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (11.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Load the model and processor\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Kaushika04/qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika\", torch_dtype=\"auto\", device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load the processor\n",
        "processor = AutoProcessor.from_pretrained(\"Kaushika04/qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika\")\n",
        "\n",
        "# Function to preprocess the image\n",
        "def preprocess_image(image_path):\n",
        "    # Load and preprocess the image using the processor\n",
        "    image = processor.images(image_path)\n",
        "    return image\n",
        "\n",
        "# Sample message structure\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"image\",\n",
        "                \"image\": \"/content/LLaMA-Factory/kaushika/human_9.jpg\"  # Path to your image\n",
        "            },\n",
        "            {\"type\": \"text\", \"text\": \"Describe?\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Preparing input for inference\n",
        "text = processor.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Process vision inputs (images)\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "# Prepare the inputs for the model\n",
        "inputs = processor(\n",
        "    text=[text],\n",
        "    images=image_inputs,\n",
        "    videos=video_inputs,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "inputs = inputs.to(\"cuda\")\n",
        "\n",
        "# Inference: Generation of the output\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
        "\n",
        "# Trim the output to exclude the input part of the tokens\n",
        "generated_ids_trimmed = [\n",
        "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "# Decode the generated output\n",
        "output_text = processor.batch_decode(\n",
        "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        ")\n",
        "\n",
        "# Print the output text\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "63863af80d2b45f08015c93c6f993d97",
            "96c6d00e1db74f00a371b286365e2a16",
            "a84ed58830ad4d84a977c391bfaa2096",
            "cff29d93d1d2497da7ad665941617c44",
            "a956922642fa4df2814658006b23f375",
            "d9ac6b1e2d4f4fd0a96d27aa9905a35c",
            "a0ec4a6181e14bf290580ded76f02bc5",
            "f932d94fca0d4fa19972f5bb56a9edce",
            "a545bc30d2ea44d48a7b4d4e7d09a768",
            "5b9a232982bd4595bd66de0a153bfd79",
            "d9d3073d743a4c0e9400b56fac43ca01"
          ]
        },
        "id": "f_znoNbXyyWx",
        "outputId": "3f18e38c-f474-4d9f-8323-a7527972cebf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63863af80d2b45f08015c93c6f993d97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is HUMAN.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import torch\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import os\n",
        "\n",
        "# Load the model and processor\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Kaushika04/qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    \"Kaushika04/qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika\"\n",
        ")\n",
        "\n",
        "# Sample message with image path and prompt\n",
        "image_path = \"/content/LLaMA-Factory/kaushika/dog_16.jpg\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"image\",\n",
        "                \"image\": image_path\n",
        "            },\n",
        "            {\"type\": \"text\", \"text\": \"Describe?\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Prepare text input\n",
        "text = processor.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Process vision inputs\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "# Tokenize inputs\n",
        "inputs = processor(\n",
        "    text=[text],\n",
        "    images=image_inputs,\n",
        "    videos=video_inputs,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Move to GPU if available\n",
        "inputs = inputs.to(\"cuda\")\n",
        "\n",
        "# Generate output\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
        "\n",
        "# Trim prompt from output tokens\n",
        "generated_ids_trimmed = [\n",
        "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "# Decode output\n",
        "output_text = processor.batch_decode(\n",
        "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        ")\n",
        "\n",
        "# ------------------------ Overlay the text on the image ------------------------\n",
        "\n",
        "# Load the image\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# Prepare to draw on image\n",
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "# Try to use a system font; fallback to default if not available\n",
        "try:\n",
        "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", size=24)\n",
        "except:\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "# Get generated text\n",
        "text = output_text[0]\n",
        "\n",
        "# Define position\n",
        "position = (10, 10)\n",
        "\n",
        "# Optional: Draw background rectangle for text readability\n",
        "text_size = draw.textbbox(position, text, font=font)\n",
        "draw.rectangle([text_size[0] - 5, text_size[1] - 5, text_size[2] + 5, text_size[3] + 5], fill=\"black\")\n",
        "\n",
        "# Draw text\n",
        "draw.text(position, text, fill=\"yellow\", font=font)\n",
        "\n",
        "# Save and show the image\n",
        "output_img_path = \"/content/output_with_text_2.jpg\"\n",
        "image.save(output_img_path)\n",
        "image.show()  # In Colab use: from IPython.display import display; display(image)\n",
        "\n",
        "# Print the raw text too\n",
        "print(\"Model output:\", text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457,
          "referenced_widgets": [
            "2d41016bf4024e3baca2ffab55c0abd0",
            "089eff09c5574737a198129f7b3f5d64",
            "1f87ba8b996d49858db7413da2b69bde",
            "e7d828c9ceb94fa3a624f7d8086ae6ae",
            "a5ae59e3e04b431a961a3d2825b47dce",
            "265c9de21cdf46d6a9cc50abeeeda9c1",
            "63cec27bd6d547a4b308449600822ad7",
            "2dc87543e69d458a9e4e2cf4e92fb9fd",
            "653a6f1f931846d29a2c84314d767d5d",
            "b7f5343ef53545ac925fc06deb294186",
            "825cba81c72044b49fb26db934f7d853"
          ]
        },
        "id": "BtZie7WYFpdA",
        "outputId": "9bc65758-5de7-4f9c-baa5-c5c7ce0c7672"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d41016bf4024e3baca2ffab55c0abd0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 11.81 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.46 GiB is free. Process 309581 has 9.28 GiB memory in use. Of the allocated memory 7.08 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-9bb45521ad23>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Generate output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mgenerated_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Trim prompt from output tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2451\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2452\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2453\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3417\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3418\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3419\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3420\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position)\u001b[0m\n\u001b[1;32m   1659\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpixel_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                 \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m                 \u001b[0mimage_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_thw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_grid_thw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mn_image_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mn_image_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, grid_thw)\u001b[0m\n\u001b[1;32m   1019\u001b[0m                 )\n\u001b[1;32m   1020\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcu_seqlens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcu_seqlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     ) -> torch.Tensor:\n\u001b[0;32m--> 430\u001b[0;31m         hidden_states = hidden_states + self.attn(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mcu_seqlens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcu_seqlens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         attn_output = F.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 11.81 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.46 GiB is free. Process 309581 has 9.28 GiB memory in use. Of the allocated memory 7.08 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Load model and processor\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Kaushika04/qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika\",\n",
        "    torch_dtype=\"auto\", device_map=\"auto\"\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    \"Kaushika04/qwen2vl_2b_instruct_lora_merged_HUMAN_OR_ANIMAL_IMAGE_FINETUNING_kaushika\"\n",
        ")\n",
        "\n",
        "# Define ground truth labels\n",
        "true_labels = {\n",
        "    \"1.jpg\": \"This is an animal - gorilla.\",\n",
        "    \"2.jpg\": \"This is an animal - gorilla.\",\n",
        "    \"3.jpg\": \"This is an animal - dog.\",\n",
        "    \"4.jpg\": \"This is an animal - dog.\",\n",
        "    \"5.jpg\": \"This is an animal - cat.\",\n",
        "    \"6.jpg\": \"This is an animal - cat.\",\n",
        "    \"7.jpg\": \"This is an animal - dog.\",\n",
        "    \"8.jpg\": \"This is an animal - gorilla.\",\n",
        "    \"9.jpg\": \"This is HUMAN.\",\n",
        "    \"10.jpg\": \"This is HUMAN.\",\n",
        "    \"11.jpg\": \"This is HUMAN.\",\n",
        "    \"12.jpg\": \"This is HUMAN.\",\n",
        "}\n",
        "\n",
        "correct = 0\n",
        "total = len(true_labels)\n",
        "\n",
        "for file_name, true_label in true_labels.items():\n",
        "    image_path = os.path.join(\"/content/LLaMA-Factory/test_dataset\", file_name)\n",
        "\n",
        "    # Decide prompt based on image number\n",
        "    img_num = int(file_name.split('.')[0])\n",
        "    question = \"What is this?\" if img_num <= 8 else \"Who is this?\"\n",
        "\n",
        "    # Prepare message for model\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image_path},\n",
        "                {\"type\": \"text\", \"text\": question}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Process inputs\n",
        "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate output\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=64)\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    output_text = processor.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )[0].lower()\n",
        "\n",
        "    # Check match\n",
        "    if true_label.lower() in output_text:\n",
        "        correct += 1\n",
        "    else:\n",
        "        print(f\"❌ Wrong prediction for {file_name}\")\n",
        "        print(f\"   ▶ Expected: {true_label}\")\n",
        "        print(f\"   ▶ Got     : {output_text.strip()}\\n\")\n",
        "\n",
        "# Final accuracy\n",
        "accuracy = (correct / total) * 100\n",
        "print(f\"\\n✅ Approximate Accuracy: {accuracy:.2f}% on {total} test images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "e9dfcd0da60a461e8c834a7b2c07ced4",
            "9dc08fc29e8d4967b6f5ceb4b9df4ccd",
            "b51873f397984dedacce5aab1af426df",
            "c3d37f84d3404236bc23e5373da3aac8",
            "6376d4af70d94514b37186fbf828b841",
            "41786e22fa214a95b382191bdc0d389b",
            "4f0b7a2b5b2f4949b5ec6a8938407d21",
            "2475359adf8742eda908218d8a433978",
            "1d35bcded09a4ddb9b331eda6d075db7",
            "95726b06b8334b7ea156f743f7ca6f54",
            "116643b7eb1b40b5af39fb30e842effa"
          ]
        },
        "id": "FyH4XJ1kIdTD",
        "outputId": "3f7013d8-3f7a-4214-bb3a-3e5d41253570"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9dfcd0da60a461e8c834a7b2c07ced4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Approximate Accuracy: 100.00% on 12 test images\n"
          ]
        }
      ]
    }
  ]
}